{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10880021,"sourceType":"datasetVersion","datasetId":6760222}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session#","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import libraries \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.optim as optim\nimport cv2\nimport numpy as np\nimport os\nfrom sklearn.metrics import precision_score, recall_score, jaccard_score\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom transformers import CLIPModel, CLIPProcessor\nimport glob\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:19:01.240630Z","iopub.execute_input":"2025-05-11T11:19:01.240863Z","iopub.status.idle":"2025-05-11T11:19:29.031435Z","shell.execute_reply.started":"2025-05-11T11:19:01.240844Z","shell.execute_reply":"2025-05-11T11:19:29.030562Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n2025-05-11 11:19:16.605716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746962356.824026      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746962356.889743      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CBAM Implementation\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass CBAM(nn.Module):\n    def __init__(self, in_planes, ratio=16, kernel_size=7):\n        super(CBAM, self).__init__()\n        self.channel_attention = ChannelAttention(in_planes, ratio)\n        self.spatial_attention = SpatialAttention(kernel_size)\n\n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:20:13.852451Z","iopub.execute_input":"2025-05-11T11:20:13.852724Z","iopub.status.idle":"2025-05-11T11:20:13.861490Z","shell.execute_reply.started":"2025-05-11T11:20:13.852704Z","shell.execute_reply":"2025-05-11T11:20:13.860773Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Configurations\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 2\nlr = 1e-3  # Updated learning rate\nnum_epochs = 200\nedge_methods = ['sobel', 'canny', 'laplacian']\nvlm_feature_size = 512\nlambda_dice = 1.0  # Weight for Dice loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:20:30.562726Z","iopub.execute_input":"2025-05-11T11:20:30.563207Z","iopub.status.idle":"2025-05-11T11:20:30.567382Z","shell.execute_reply.started":"2025-05-11T11:20:30.563181Z","shell.execute_reply":"2025-05-11T11:20:30.566670Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load CLIP model and processor\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:20:58.492898Z","iopub.execute_input":"2025-05-11T11:20:58.493575Z","iopub.status.idle":"2025-05-11T11:21:04.653701Z","shell.execute_reply.started":"2025-05-11T11:20:58.493540Z","shell.execute_reply":"2025-05-11T11:21:04.653097Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d34700b4a604ad89a416109b69d0058"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2af010f4244220ae9409f33fb8811e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b529036ac74d12ab044928c4f79936"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45649f8c1c1f42d19469e37451f56282"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31a0e8a69f514a81846a2cb51c3387c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d53ec76240444ab7859bdc813e66d223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5cba8169f044dc08b9964edb8737862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2904e551d4a44880bf60a8265033258f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c852628d13149cd965b532d831f7e7b"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#extracting VLM features\ndef extract_vlm_features(image):\n    if image.ndim == 2:\n        image = np.stack([image]*3, axis=-1)\n    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        features = clip_model.get_image_features(**inputs)\n    return features.cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:21:26.912907Z","iopub.execute_input":"2025-05-11T11:21:26.913195Z","iopub.status.idle":"2025-05-11T11:21:26.917919Z","shell.execute_reply.started":"2025-05-11T11:21:26.913175Z","shell.execute_reply":"2025-05-11T11:21:26.917057Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Combined edge map\ndef get_edge_map(image, methods=['sobel', 'canny', 'laplacian']):\n    edge_maps = []\n    for method in methods:\n        if method == 'canny':\n            edge_map = cv2.Canny(image, 50, 150)\n        elif method == 'sobel':\n            sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n            sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n            edge_map = np.sqrt(sobelx**2 + sobely**2)\n        elif method == 'laplacian':\n            edge_map = cv2.Laplacian(image, cv2.CV_64F, ksize=3)\n        edge_maps.append(edge_map)\n    combined_edge_map = np.mean(np.array(edge_maps), axis=0)\n    combined_edge_map = np.abs(combined_edge_map)\n    combined_edge_map = (combined_edge_map - combined_edge_map.min()) / (combined_edge_map.max() - combined_edge_map.min() + 1e-8)\n    return combined_edge_map.astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:22:06.142468Z","iopub.execute_input":"2025-05-11T11:22:06.143044Z","iopub.status.idle":"2025-05-11T11:22:06.148777Z","shell.execute_reply.started":"2025-05-11T11:22:06.143022Z","shell.execute_reply":"2025-05-11T11:22:06.147979Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#preprocessing\n\n# Transforms\ntrain_transform = A.Compose([\n    A.Resize(256, 256),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(256, 256),\n    ToTensorV2()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:22:30.287740Z","iopub.execute_input":"2025-05-11T11:22:30.288028Z","iopub.status.idle":"2025-05-11T11:22:30.297478Z","shell.execute_reply.started":"2025-05-11T11:22:30.288005Z","shell.execute_reply":"2025-05-11T11:22:30.296878Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Dataset Class\nclass TNBCDataset(Dataset):\n    def __init__(self, image_paths, mask_paths, transform=None, edge_methods=['sobel']):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n        self.edge_methods = edge_methods\n\n    def __getitem__(self, idx):\n        image = cv2.imread(self.image_paths[idx], cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n        edges = get_edge_map(image, self.edge_methods)\n        combined_image = np.stack([image, edges], axis=-1)\n        image_rgb = np.stack([image]*3, axis=-1)\n        vlm_features = extract_vlm_features(image_rgb)\n\n        if self.transform:\n            augmented = self.transform(image=combined_image, mask=mask)\n            combined_image = augmented['image']\n            mask = augmented['mask'].float() / 255.0\n        else:\n            combined_image = torch.from_numpy(combined_image.transpose(2, 0, 1)).float()\n            mask = torch.from_numpy(mask).float() / 255.0\n\n        if mask.dim() == 2:\n            mask = mask.unsqueeze(0)\n\n        vlm_features = torch.tensor(vlm_features, dtype=torch.float32).squeeze(0)\n        return combined_image, mask, vlm_features\n\n    def __len__(self):\n        return len(self.image_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:23:01.283720Z","iopub.execute_input":"2025-05-11T11:23:01.284343Z","iopub.status.idle":"2025-05-11T11:23:01.290952Z","shell.execute_reply.started":"2025-05-11T11:23:01.284320Z","shell.execute_reply":"2025-05-11T11:23:01.290282Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Model Architecture\nclass AttentionUNet(nn.Module):\n    def __init__(self, in_channels=2, out_channels=1, vlm_feature_size=512):\n        super().__init__()\n        self.enc1 = self.conv_block(in_channels, 64)\n        self.cbam1 = CBAM(64)\n        self.enc2 = self.conv_block(64, 128)\n        self.cbam2 = CBAM(128)\n        self.enc3 = self.conv_block(128, 256)\n        self.cbam3 = CBAM(256)\n        self.enc4 = self.conv_block(256, 512)\n        self.cbam4 = CBAM(512)\n        self.pool = nn.MaxPool2d(2)\n\n        self.vlm_proj1 = nn.Sequential(\n            nn.Linear(vlm_feature_size, 64),\n            nn.Unflatten(1, (64, 1, 1)))\n        self.vlm_proj2 = nn.Sequential(\n            nn.Linear(vlm_feature_size, 128),\n            nn.Unflatten(1, (128, 1, 1)))\n        self.vlm_proj3 = nn.Sequential(\n            nn.Linear(vlm_feature_size, 256),\n            nn.Unflatten(1, (256, 1, 1)))\n        self.vlm_proj4 = nn.Sequential(\n            nn.Linear(vlm_feature_size, 512),\n            nn.Unflatten(1, (512, 1, 1)))\n\n        self.dec1 = self.upconv_block(512, 256)\n        self.att1 = AttentionBlock(256, 256, vlm_dim=256)\n        self.dec2 = self.upconv_block(512, 128)\n        self.att2 = AttentionBlock(128, 128, vlm_dim=128)\n        self.dec3 = self.upconv_block(256, 64)\n        self.att3 = AttentionBlock(64, 64, vlm_dim=64)\n        self.final = nn.Conv2d(128, out_channels, 1)\n\n    def conv_block(self, in_c, out_c):\n        return nn.Sequential(\n            nn.Conv2d(in_c, out_c, 3, padding=1),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU(),\n            nn.Conv2d(out_c, out_c, 3, padding=1),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU()\n        )\n\n    def upconv_block(self, in_c, out_c):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_c, out_c, 2, 2),\n            nn.BatchNorm2d(out_c),\n            nn.ReLU()\n        )\n\n    def forward(self, x, vlm_features):\n        vlm1 = self.vlm_proj1(vlm_features)\n        vlm2 = self.vlm_proj2(vlm_features)\n        vlm3 = self.vlm_proj3(vlm_features)\n        vlm4 = self.vlm_proj4(vlm_features)\n\n        e1 = self.enc1(x)\n        e1 = self.cbam1(e1)\n        e2 = self.enc2(self.pool(e1))\n        e2 = self.cbam2(e2)\n        e3 = self.enc3(self.pool(e2))\n        e3 = self.cbam3(e3)\n        e4 = self.enc4(self.pool(e3))\n        e4 = self.cbam4(e4)\n\n        d1 = self.dec1(e4)\n        a1 = self.att1(d1, e3, vlm3)\n        d1 = torch.cat([a1, d1], dim=1)\n\n        d2 = self.dec2(d1)\n        a2 = self.att2(d2, e2, vlm2)\n        d2 = torch.cat([a2, d2], dim=1)\n\n        d3 = self.dec3(d2)\n        a3 = self.att3(d3, e1, vlm1)\n        d3 = torch.cat([a3, d3], dim=1)\n\n        return torch.sigmoid(self.final(d3))\n\n#Attention module\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, F_g, F_l, vlm_dim):\n        super().__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_l, 1),\n            nn.BatchNorm2d(F_l)\n        )\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l + vlm_dim, F_l, 1),\n            nn.BatchNorm2d(F_l)\n        )\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_l, 1, 1),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        self.relu = nn.ReLU()\n\n    def forward(self, g, x, vlm):\n        vlm_spatial = F.interpolate(vlm, size=x.shape[2:], mode='bilinear')\n        x_vlm = torch.cat([x, vlm_spatial], dim=1)\n        g1 = self.W_g(g)\n        x1 = self.W_x(x_vlm)\n        if g1.shape[2:] != x1.shape[2:]:\n            g1 = F.interpolate(g1, size=x1.shape[2:], mode='bilinear')\n        psi = self.relu(g1 + x1)\n        return x * self.psi(psi)\n\n#Discriminator\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=2):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(in_channels, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 4, 1, 1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1, 4, 1, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:26:12.335471Z","iopub.execute_input":"2025-05-11T11:26:12.336182Z","iopub.status.idle":"2025-05-11T11:26:12.352292Z","shell.execute_reply.started":"2025-05-11T11:26:12.336156Z","shell.execute_reply":"2025-05-11T11:26:12.351669Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Initialize models and optimizers\ngenerator = AttentionUNet(vlm_feature_size=vlm_feature_size).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss functions\nbce_loss = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.0):\n    pred = pred.contiguous().view(pred.shape[0], -1)\n    target = target.contiguous().view(target.shape[0], -1)\n    intersection = (pred * target).sum(dim=1)\n    dice = (2. * intersection + smooth) / (pred.sum(dim=1) + target.sum(dim=1) + smooth)\n    return 1 - dice.mean()\n\nopt_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\nopt_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:26:53.938837Z","iopub.execute_input":"2025-05-11T11:26:53.939578Z","iopub.status.idle":"2025-05-11T11:26:54.035556Z","shell.execute_reply.started":"2025-05-11T11:26:53.939548Z","shell.execute_reply":"2025-05-11T11:26:54.034726Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Metrics and Data Preparation\ndef calculate_metrics(pred, target):\n    pred_bin = (pred > 0.5).float()\n    target = target.cpu().numpy().astype(np.uint8)\n    pred_bin = pred_bin.cpu().numpy().astype(np.uint8)\n    dice = (2 * (pred_bin * target).sum()) / (pred_bin.sum() + target.sum() + 1e-8)\n    iou = jaccard_score(target.flatten(), pred_bin.flatten(), average='binary')\n    precision = precision_score(target.flatten(), pred_bin.flatten(), average='binary', zero_division=0)\n    recall = recall_score(target.flatten(), pred_bin.flatten(), average='binary')\n    return dice, iou, precision, recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:27:08.913709Z","iopub.execute_input":"2025-05-11T11:27:08.914502Z","iopub.status.idle":"2025-05-11T11:27:08.919550Z","shell.execute_reply.started":"2025-05-11T11:27:08.914475Z","shell.execute_reply":"2025-05-11T11:27:08.918902Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Dataset Paths\ntrain_image_dir = \"/kaggle/input/ph2segmentation/Train/Images\"\ntrain_mask_dir = \"/kaggle/input/ph2segmentation/Train/Masks\"\nval_image_dir = \"/kaggle/input/ph2segmentation/Validation/Images\"\nval_mask_dir = \"/kaggle/input/ph2segmentation/Validation/Masks\"\ntest_image_dir = \"/kaggle/input/ph2segmentation/Test/Images\"\ntest_mask_dir = \"/kaggle/input/ph2segmentation/Test/Masks\"\n\ntrain_image_paths = sorted(glob.glob(os.path.join(train_image_dir, \"*\")))\ntrain_mask_paths = sorted(glob.glob(os.path.join(train_mask_dir, \"*\")))\nval_image_paths = sorted(glob.glob(os.path.join(val_image_dir, \"*\")))\nval_mask_paths = sorted(glob.glob(os.path.join(val_mask_dir, \"*\")))\ntest_image_paths = sorted(glob.glob(os.path.join(test_image_dir, \"*\")))\ntest_mask_paths = sorted(glob.glob(os.path.join(test_mask_dir, \"*\")))\n\ntrain_dataset = TNBCDataset(train_image_paths, train_mask_paths, train_transform, edge_methods)\nval_dataset = TNBCDataset(val_image_paths, val_mask_paths, val_transform, edge_methods)\ntest_dataset = TNBCDataset(test_image_paths, test_mask_paths, val_transform, edge_methods)\n\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size)\ntest_loader = DataLoader(test_dataset, batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:27:25.369584Z","iopub.execute_input":"2025-05-11T11:27:25.370324Z","iopub.status.idle":"2025-05-11T11:27:25.433214Z","shell.execute_reply.started":"2025-05-11T11:27:25.370298Z","shell.execute_reply":"2025-05-11T11:27:25.432672Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Training, Validation, Testing\n\n# Training Loop\nbest_val_dice = 0\nfor epoch in range(num_epochs):\n    generator.train()\n    discriminator.train()\n    running_metrics = {'dice': 0, 'iou': 0, 'precision': 0, 'recall': 0}\n\n    for images, masks, vlm_features in tqdm(train_loader):\n        images, masks = images.to(device), masks.to(device)\n        vlm_features = vlm_features.to(device)\n\n        if masks.dim() == 3:\n            masks = masks.unsqueeze(1)\n\n        # Train Discriminator\n        opt_d.zero_grad()\n\n        # Real samples\n        real_input = torch.cat([images[:, :1, :, :], masks], dim=1)\n        real_pred = discriminator(real_input)\n        real_loss = bce_loss(real_pred, torch.ones_like(real_pred))\n\n        # Fake samples\n        with torch.no_grad():\n            fake_masks = generator(images, vlm_features)\n        fake_input = torch.cat([images[:, :1, :, :], fake_masks], dim=1)\n        fake_pred = discriminator(fake_input.detach())\n        fake_loss = bce_loss(fake_pred, torch.zeros_like(fake_pred))\n\n        d_loss = (real_loss + fake_loss) / 2\n        d_loss.backward()\n        opt_d.step()\n        # Train Generator\n        opt_g.zero_grad()\n        fake_masks = generator(images, vlm_features)\n\n        # Dice Loss\n        dice = dice_loss(fake_masks, masks)\n\n        # Adversarial loss\n        fake_input = torch.cat([images[:, :1, :, :], fake_masks], dim=1)\n        g_pred = discriminator(fake_input)\n        g_loss = bce_loss(g_pred, torch.ones_like(g_pred))\n\n        # Total generator loss\n        total_loss = g_loss + lambda_dice * dice\n        total_loss.backward()\n        opt_g.step()\n\n        # Calculate metrics\n        with torch.no_grad():\n            dice, iou, prec, rec = calculate_metrics(fake_masks, masks)\n            running_metrics['dice'] += dice\n            running_metrics['iou'] += iou\n            running_metrics['precision'] += prec\n            running_metrics['recall'] += rec\n    # Validation\n    generator.eval()\n    val_metrics = {'dice': 0, 'iou': 0, 'precision': 0, 'recall': 0}\n    with torch.no_grad():\n        for images, masks, vlm_features in val_loader:\n            images, masks = images.to(device), masks.to(device)\n            vlm_features = vlm_features.to(device)\n            fake_masks = generator(images, vlm_features)\n            dice, iou, prec, rec = calculate_metrics(fake_masks, masks)\n            val_metrics['dice'] += dice\n            val_metrics['iou'] += iou\n            val_metrics['precision'] += prec\n            val_metrics['recall'] += rec\n\n    val_dice = val_metrics['dice'] / len(val_loader)\n    if val_dice > best_val_dice:\n        best_val_dice = val_dice\n        torch.save(generator.state_dict(), 'best_generator.pth')\n\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Dice: {running_metrics['dice']/len(train_loader):.4f}\")\n    print(f\"Val Dice: {val_dice:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing\ngenerator.load_state_dict(torch.load('best_generator.pth'))\ngenerator.eval()\ntest_metrics = {'dice': 0, 'iou': 0, 'precision': 0, 'recall': 0}\n\nwith torch.no_grad():\n    for images, masks, vlm_features in test_loader:\n        images, masks = images.to(device), masks.to(device)\n        vlm_features = vlm_features.to(device)\n        fake_masks = generator(images, vlm_features)\n        dice, iou, prec, rec = calculate_metrics(fake_masks, masks)\n        test_metrics['dice'] += dice\n        test_metrics['iou'] += iou\n        test_metrics['precision'] += prec\n        test_metrics['recall'] += rec\n\nprint(\"\\nFinal Test Results:\")\nprint(f\"Dice: {test_metrics['dice']/len(test_loader):.4f}\")\nprint(f\"IoU: {test_metrics['iou']/len(test_loader):.4f}\")\nprint(f\"Precision: {test_metrics['precision']/len(test_loader):.4f}\")\nprint(f\"Recall: {test_metrics['recall']/len(test_loader):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}